{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AarishB/FragrAI/blob/main/FragrAIi_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpNht0png3v4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from IPython.display import display\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# === STEP 1: Load and clean dataset ===\n",
        "df = pd.read_csv('perfumes_table.csv', nrows=20000)\n",
        "df = df.drop(columns=['url', 'rating', 'reviews'], errors='ignore')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Reorder columns in the order we see please\n",
        "desired_order = ['title', 'notes', 'description']\n",
        "remaining_cols = [col for col in df.columns if col not in desired_order]\n",
        "df = df[desired_order + remaining_cols]\n",
        "\n",
        "# Capitalize notes\n",
        "def capitalize(s):\n",
        "    if not isinstance(s, str): return s\n",
        "    exceptions = {'and', 'for', 'of', 'the', 'in', 'on', 'at', 'with', 'a', 'an'}\n",
        "    words = s.split()\n",
        "    return ' '.join([\n",
        "        word.capitalize() if i == 0 or word not in exceptions else word\n",
        "        for i, word in enumerate(words)\n",
        "    ])\n",
        "df['designer'] = df['designer'].apply(capitalize)\n",
        "\n",
        "def strip_accents(s): #turns the notes into unicode to start stripping unneccesary patterns\n",
        "    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))\n",
        "\n",
        "def canon_note(x): #these two methods help make unusual notes appear readable, used help from ChatGPT to get a regex pattern.\n",
        "    if not isinstance(x, str): return None\n",
        "    n = strip_accents(x).lower()\n",
        "    n = re.sub(r'[^a-z0-9\\s]', ' ', n).strip()\n",
        "\n",
        "\n",
        "   #Takes cares of plurals and phrases of notes\n",
        "    n = re.sub(r'\\bnotes?\\b', '', n).strip()\n",
        "    n = re.sub(r'\\bwoody notes?\\b', 'woody', n)\n",
        "    n = re.sub(r'\\bgreen notes?\\b', 'green', n)\n",
        "    n = re.sub(r'\\bfruity notes?\\b', 'fruity', n)\n",
        "    n = re.sub(r'\\bfloral notes?\\b', 'floral', n)\n",
        "    n = re.sub(r'\\bspicy notes?\\b', 'spicy', n)\n",
        "    n = re.sub(r'\\baromatic notes?\\b', 'aromatic', n)\n",
        "    n = re.sub(r'\\bsweet notes?\\b', 'sweet', n)\n",
        "    synonyms = {\n",
        "        'calabrian bergamot':'bergamot','sicilian bergamot':'bergamot','italian bergamot':'bergamot',\n",
        "        'cedarwood':'cedar','virginia cedar':'cedar','atlas cedar':'cedar',\n",
        "        'oudh':'oud','agarwood':'oud','oud wood':'oud',\n",
        "        'pink pepper':'pepper','black pepper':'pepper',\n",
        "        'white musk':'musk','ambroxan':'amber','ambergris':'amber',\n",
        "        'sea notes':'marine','marine notes':'marine','watery notes':'marine','aqua':'marine','calone':'marine',\n",
        "        'orange blossom':'neroli','orris root':'iris','cashmeran':'woody'\n",
        "    } #Start of the synonyms, more to come soon\n",
        "    n = synonyms.get(n, n)\n",
        "    return ' '.join(w.capitalize() for w in n.split())\n",
        "\n",
        "def _extract_layer(text, layer_aliases, stop_regex): #Notes will be extracted layer by layer, not all at once. GPT provided an efficient pattern\n",
        "   def _extract_layer(text, layer_aliases, stop_regex):\n",
        "    \"\"\"\n",
        "    Extract a comma/semicolon/pipe separated list of notes after a layer alias.\n",
        "    Adds more natural-language boundaries so we don't stop too late/too early.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    s = ' '.join(text.split())  # flatten whitespace\n",
        "\n",
        "    # Add richer \"stop\" phrases that often separate layers\n",
        "    # (e.g., \"followed by\", \"transitioning into\", \"settling into\", etc.)\n",
        "    extra_stops = r\"|\".join([\n",
        "        r\"\\bfollowed by\\b\",\n",
        "        r\"\\btransition(?:ing)? into\\b\",\n",
        "        r\"\\bsettling (?:down|into)\\b\",\n",
        "        r\"\\bsupported by\\b\",\n",
        "        r\"\\bgrounded in\\b\",\n",
        "        r\"\\banchored by\\b\",\n",
        "        r\"\\bon (?:a|the) (?:bed|base) of\\b\",\n",
        "        r\"\\bthen\\b\",  # often used as a weak layer separator\n",
        "    ])\n",
        "\n",
        "    # Merge with provided stop regex\n",
        "    full_stop = rf\"(?:{stop_regex}|{extra_stops}|\\.|$)\"\n",
        "\n",
        "    # e.g., \"Top notes are: A, B and C\" / \"Opening: A; B | C\"\n",
        "    pat = rf\"\\b({'|'.join(layer_aliases)})\\s*(?:note|notes)?\\s*(?:are|include|:)?\\s*(.*?)(?={full_stop})\"\n",
        "    m = re.search(pat, s, flags=re.IGNORECASE)\n",
        "    if not m:\n",
        "        return []\n",
        "    chunk = m.group(2)\n",
        "    toks = re.split(r'[;,/|]', chunk)\n",
        "    return [canon_note(t) for t in toks if canon_note(t)]\n",
        "\n",
        "def extract_notes(desc, note_type): #Better extraction of notes, which is more efficient and structured.\n",
        "    aliases = {\n",
        "        'Top': ['top','opening','head'],\n",
        "        'Middle': ['middle','heart', 'core'],\n",
        "        'Base': ['base','drydown', 'foundation']\n",
        "    }\n",
        "    if note_type == 'Top':\n",
        "        stop = r\"\\b(?:middle|heart|base|drydown)\\b\"\n",
        "    elif note_type == 'Middle':\n",
        "        stop = r\"\\b(?:base|drydown)\\b\"\n",
        "    else:\n",
        "        # match until period or end (stop regex that never matches)\n",
        "        stop = r\"$^\"\n",
        "\n",
        "    return _extract_layer(desc, aliases[note_type], stop)\n",
        "\n",
        "# Re-run extraction with the new regex pattern\n",
        "df['top_notes']    = df['description'].apply(lambda x: extract_notes(x, \"Top\"))\n",
        "df['middle_notes'] = df['description'].apply(lambda x: extract_notes(x, \"Middle\"))\n",
        "df['base_notes']   = df['description'].apply(lambda x: extract_notes(x, \"Base\"))\n",
        "\n",
        "if 'notes' in df.columns: #tokes to maximize outputs\n",
        "    def fallback(row):\n",
        "        if row['top_notes'] or row['middle_notes'] or row['base_notes']:\n",
        "            return row\n",
        "        raw = row['notes']\n",
        "        toks = []\n",
        "        if isinstance(raw, str):\n",
        "            toks = [canon_note(t) for t in re.split(r'[;,/|]', raw) if canon_note(t)]\n",
        "        elif isinstance(raw, (list, tuple)):\n",
        "            toks = [canon_note(t) for t in raw if canon_note(t)]\n",
        "\n",
        "        if toks:\n",
        "            k = len(toks)\n",
        "            if k >= 3:\n",
        "                t1 = toks[:max(1, k//3)]\n",
        "                t2 = toks[max(1, k//3):max(2, 2*k//3)]\n",
        "                t3 = toks[max(2, 2*k//3):]\n",
        "            elif k == 2:\n",
        "                t1, t2, t3 = [toks[0]], [], [toks[1]]\n",
        "            else:  # k == 1\n",
        "                t1, t2, t3 = [toks[0]], [], []\n",
        "            row['top_notes'], row['middle_notes'], row['base_notes'] = t1, t2, t3\n",
        "        return row\n",
        "    df = df.apply(fallback, axis=1)\n",
        "\n",
        "# === STEP 3: Create combined notes with weight for base notes ===\n",
        "for col in ['top_notes', 'middle_notes', 'base_notes']:\n",
        "    df[col] = df[col].apply(lambda v: v if isinstance(v, list) else ([] if pd.isna(v) or v is None else [v] if isinstance(v, str) else []))\n",
        "df['combined_notes'] = df.apply(lambda row: row['top_notes'] + row['middle_notes'] + row['base_notes'] * 2, axis=1)\n",
        "\n",
        "# === STEP 4: Assign seasons for supervised learning ===\n",
        "seasonal_notes = {\n",
        "    'Summer': ['Citrus', 'Aquatic', 'Mint', 'Neroli', 'Grapefruit', 'Orange', 'Lemon', 'Bergamot', 'Melon', 'Pineapple', 'Coconut'],\n",
        "    'Winter': ['Vanilla', 'Amber', 'Oud', 'Leather', 'Tobacco', 'Cinnamon', 'Tonka','Incense','Coffee'],\n",
        "    'Spring': ['Jasmine', 'Rose', 'Green Tea', 'Lily', 'Pear', 'Apple', 'Lavender', 'Magnolia'],\n",
        "    'Fall': ['Sandalwood', 'Patchouli', 'Nutmeg', 'Cardamom', 'Clove', 'Plum', 'Lavender', 'Cinnamon']\n",
        "}\n",
        "#weights the notes based on the layers(base gets more weightage u feel)\n",
        "def assign_weighted_season_from_layers(top, mid, base):\n",
        "    season_scores = {season: 0 for season in seasonal_notes}\n",
        "    total_notes = len(top) + len(mid) + len(base)\n",
        "\n",
        "    if total_notes == 0:\n",
        "        return 'Inconclusive'\n",
        "\n",
        "    top_wt = max(1, round((len(top)/total_notes) * 10))\n",
        "    mid_wt = max(1, round((len(mid)/total_notes) * 10))\n",
        "    base_wt = max(1, round((len(base)/total_notes) * 10))\n",
        "\n",
        "    for note in top:\n",
        "        for season, group in seasonal_notes.items():\n",
        "            if note in group:\n",
        "                season_scores[season] += (top_wt*2)\n",
        "    for note in mid:\n",
        "        for season, group in seasonal_notes.items():\n",
        "            if note in group:\n",
        "                season_scores[season] += (mid_wt*1.5)\n",
        "    for note in base:\n",
        "        for season, group in seasonal_notes.items():\n",
        "            if note in group:\n",
        "                season_scores[season] += (base_wt*2.5)\n",
        "\n",
        "    return max(season_scores, key=season_scores.get) if any(season_scores.values()) else 'Inconclusive'\n",
        "\n",
        "df['season'] = df.apply(\n",
        "    lambda row: assign_weighted_season_from_layers(row['top_notes'], row['middle_notes'], row['base_notes']),\n",
        "    axis=1\n",
        ")\n",
        "df = df[df['season'] != 'Inconclusive']\n",
        "\n",
        "# === STEP 5: Train ML model ===\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "mlb = MultiLabelBinarizer()\n",
        "X = mlb.fit_transform(df['combined_notes'])\n",
        "y = df['season']\n",
        "\n",
        "X_bal, y_bal = ros.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n",
        "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
        ")\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    objective=\"multi:softprob\",   # multiclass probabilities\n",
        "    eval_metric=\"mlogloss\"\n",
        ")\n",
        "xgb.fit(X_train_xgb, y_train_xgb)\n",
        "y_pred_xgb = le.inverse_transform(xgb.predict(X_test_xgb))\n",
        "\n",
        "# LinearSVC -> Can help reduce inconvlusives\n",
        "_base_svm = LinearSVC(C=1.0, class_weight='balanced', random_state=42)\n",
        "svm = CalibratedClassifierCV(_base_svm, cv=5)  # sigmoid by default\n",
        "svm.fit(X, y)\n",
        "\n",
        "# Evaluate SVM\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"\\n[SVM] Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"\\n[SVM] Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# === STEP 6: Evaluate model ===\n",
        "y_pred_svm = svm.predict(X_test)  # use the calibrated SVM you actually fit\n",
        "print(\"\\n[SVM] Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"\\n[SVM] Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# === STEP 7: Predict on full dataset and show notes ===\n",
        "df['predicted_season_svm_raw'] = svm.predict(X)\n",
        "df['predicted_season'] = df[\"predicted_season_svm_raw\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Restricts the SVM to choose a season\n",
        "REAL_SEASONS = ['Spring', 'Summer', 'Fall', 'Winter']\n",
        "\n",
        "def restrict_to_real_seasons(model, X, real_labels=REAL_SEASONS):\n",
        "    # Get class order and probabilities\n",
        "    classes = model.classes_\n",
        "    proba = model.predict_proba(X)  # works for calibrated SVM and RF\n",
        "\n",
        "    # Indices of the allowed classes\n",
        "    keep_idx = [i for i, c in enumerate(classes) if c in real_labels]\n",
        "    allowed_classes = classes[keep_idx]\n",
        "    allowed_proba = proba[:, keep_idx]\n",
        "\n",
        "    # Argmax over allowed only\n",
        "    best_idx = allowed_proba.argmax(axis=1)\n",
        "    return allowed_classes[best_idx]\n",
        "df['predicted_season_svm'] = restrict_to_real_seasons(svm, X, REAL_SEASONS)\n",
        "df['predicted_season_rf_no_inc'] = restrict_to_real_seasons(clf, X, REAL_SEASONS)\n",
        "\n",
        "\n",
        "# === Evaluate individual models ===\n",
        "print(\"SVM acc:\", svm.score(X_test, y_test))\n",
        "print(\"RF acc:\", clf.score(X_test, y_test))\n",
        "\n",
        "# === Ensemble ===\n",
        "p_svm = svm.predict_proba(X_test)\n",
        "p_rf = clf.predict_proba(X_test)\n",
        "p_ens = (0.6 * p_svm + 0.4 * p_rf)\n",
        "y_pred = [svm.classes_[i] for i in p_ens.argmax(axis=1)]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"Ensemble acc:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === STEP 8: Display results with notes used ===\n",
        "pd.set_option('display.max_rows', 5000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "display(df[['title', 'designer', 'top_notes', 'middle_notes', 'base_notes', 'combined_notes', 'predicted_season']].head(500))\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# =========================\n",
        "# === ADD-ON: V2 PIPELINE ===\n",
        "# (Appended AFTER your current code; nothing above is modified)\n",
        "# =========================\n",
        "# ======================================================================\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# --- A) Extra semantic features (keep originals intact) ---\n",
        "\n",
        "# 1) Olfactory families from description\n",
        "families_vocab = [\n",
        "    'floral','amber','citrus','woody','spicy','aromatic','fresh','oriental',\n",
        "    'green','fruity','gourmand','leather','aquatic','powdery','smoky','earthy'\n",
        "]\n",
        "def extract_families(desc):\n",
        "    if not isinstance(desc, str): return []\n",
        "    s = desc.lower()\n",
        "    return [w.capitalize() for w in families_vocab if w in s]\n",
        "\n",
        "df['families'] = df['description'].apply(extract_families)\n",
        "\n",
        "# Combine into a NEW list column (keep your original intact)\n",
        "df['combined_notes_plus'] = df['combined_notes'] + df['families']\n",
        "\n",
        "# 2) === Build a LEAK-FREE designer_bias using a stratified split mask ===\n",
        "# Create row ids to keep order stable\n",
        "df['_row_id'] = np.arange(len(df))\n",
        "\n",
        "# Use sklearn's stratified split (pandas.sample doesn't support 'stratify')\n",
        "train_ids, test_ids = train_test_split(\n",
        "    df['_row_id'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['season']\n",
        ")\n",
        "\n",
        "train_mask = np.zeros(len(df), dtype=bool)\n",
        "train_mask[train_ids] = True\n",
        "\n",
        "# Compute designer's most common season *on training rows only*\n",
        "designer_mode_train = (\n",
        "    df.loc[train_mask].groupby('designer')['season']\n",
        "      .agg(lambda x: Counter(x).most_common(1)[0][0])\n",
        "      .to_dict()\n",
        ")\n",
        "\n",
        "df['designer_bias'] = df['designer'].map(designer_mode_train).fillna('Unknown')\n",
        "\n",
        "# 3) Warm / Cool ratios and layer counts (domain priors)\n",
        "warm_notes = {'Amber','Vanilla','Oud','Leather','Tobacco','Cinnamon','Sandalwood','Tonka','Incense','Coffee','Myrrh'}\n",
        "cool_notes = {'Citrus','Mint','Marine','Green','Lavender','Grapefruit','Bergamot','Neroli','Aqua','Calone','Melon'}\n",
        "\n",
        "def ratio_counts(notes, pos_set):\n",
        "    n = len(notes) if isinstance(notes, list) else 0\n",
        "    if n == 0: return 0.0\n",
        "    return sum(1 for t in notes if t in pos_set) / n\n",
        "\n",
        "df['warm_ratio'] = df['combined_notes_plus'].apply(lambda n: ratio_counts(n, warm_notes))\n",
        "df['cool_ratio'] = df['combined_notes_plus'].apply(lambda n: ratio_counts(n, cool_notes))\n",
        "df['top_count'] = df['top_notes'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "df['mid_count'] = df['middle_notes'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "df['base_count'] = df['base_notes'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "df['note_count'] = df['combined_notes_plus'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "\n",
        "# --- B) Build NEW features (TF-IDF over notes + categorical + numeric) ---\n",
        "df['note_string_plus'] = df['combined_notes_plus'].apply(lambda lst: ' '.join(lst))\n",
        "\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=3)\n",
        "X_tfidf = tfidf.fit_transform(df['note_string_plus'])\n",
        "\n",
        "# Categorical one-hots (designer, designer_bias)\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=True)\n",
        "X_cat = ohe.fit_transform(df[['designer','designer_bias']])\n",
        "\n",
        "# Numeric block\n",
        "num_cols = ['warm_ratio','cool_ratio','top_count','mid_count','base_count','note_count']\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_num = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Final augmented matrix\n",
        "X_v2 = sp.hstack([X_tfidf, X_cat, X_num], format='csr')\n",
        "y_v2 = df['season'].values\n",
        "\n",
        "# --- C) Use the same indices from the mask for train/test split ---\n",
        "train_row_idx = df.index[train_mask].to_numpy()\n",
        "test_row_idx  = df.index[~train_mask].to_numpy()\n",
        "\n",
        "X_train_v2 = X_v2[train_row_idx, :]\n",
        "X_test_v2  = X_v2[test_row_idx, :]\n",
        "y_train_v2 = y_v2[train_row_idx]\n",
        "y_test_v2  = y_v2[test_row_idx]\n",
        "\n",
        "# --- D) Models (new names so we don't touch yours) ---\n",
        "rf_v2 = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    class_weight='balanced_subsample',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Encode for XGB only\n",
        "le_v2 = LabelEncoder()\n",
        "y_enc_v2 = le_v2.fit_transform(y_v2)\n",
        "\n",
        "# XGB will use the same split indices for fairness\n",
        "X_tr_xgb_v2 = X_v2[train_row_idx, :]\n",
        "X_te_xgb_v2 = X_v2[test_row_idx, :]\n",
        "y_tr_xgb_v2 = y_enc_v2[train_row_idx]\n",
        "y_te_xgb_v2 = y_enc_v2[test_row_idx]\n",
        "\n",
        "xgb_v2 = XGBClassifier(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    objective=\"multi:softprob\",\n",
        "    eval_metric=\"mlogloss\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_v2.fit(X_tr_xgb_v2, y_tr_xgb_v2)\n",
        "\n",
        "# Calibrated LinearSVC (fresh fit on V2 features)\n",
        "_base_svm_v2 = LinearSVC(C=1.0, class_weight='balanced', random_state=42)\n",
        "svm_v2 = CalibratedClassifierCV(_base_svm_v2, cv=5)\n",
        "svm_v2.fit(X_train_v2, y_train_v2)\n",
        "\n",
        "# Quick individual evals\n",
        "print(\"\\n[V2] RF accuracy:\", rf_v2.fit(X_train_v2, y_train_v2).score(X_test_v2, y_test_v2))\n",
        "y_pred_xgb_v2 = xgb_v2.predict(X_te_xgb_v2)\n",
        "acc_xgb_v2 = (y_pred_xgb_v2 == y_te_xgb_v2).mean()\n",
        "print(\"\\n[V2] XGB accuracy:\", acc_xgb_v2)\n",
        "print(\"\\n[V2] SVM accuracy:\", svm_v2.score(X_test_v2, y_test_v2))\n",
        "\n",
        "# --- E) Stacking ensemble (RF + XGB + SVM -> LogisticRegression) ---\n",
        "# Wrapper so XGB fits the StackingClassifier API with predict_proba over string labels\n",
        "class XGBProbaWrapper:\n",
        "    def __init__(self, model, label_encoder):\n",
        "        self.model = model\n",
        "        self.le = label_encoder\n",
        "    def fit(self, X, y):\n",
        "        y_enc = self.le.transform(y)\n",
        "        self.model.fit(X, y_enc)\n",
        "        return self\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X)\n",
        "    def predict(self, X):\n",
        "        return self.le.inverse_transform(np.argmax(self.predict_proba(X), axis=1))\n",
        "    def score(self, X, y):\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        return accuracy_score(y, self.predict(X))\n",
        "\n",
        "xgb_wrap_v2 = XGBProbaWrapper(xgb_v2, le_v2)\n",
        "\n",
        "estimators_v2 = [\n",
        "    ('rf', rf_v2),\n",
        "    ('svm', svm_v2),\n",
        "    ('xgb', xgb_wrap_v2)\n",
        "]\n",
        "\n",
        "stack_v2 = StackingClassifier(\n",
        "    estimators=estimators_v2,\n",
        "    final_estimator=LogisticRegression(max_iter=400, class_weight='balanced'),\n",
        "    passthrough=False,\n",
        "    stack_method='predict_proba',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stack_v2.fit(X_train_v2, y_train_v2)\n",
        "print(\"\\n[V2] Stacked accuracy:\", stack_v2.score(X_test_v2, y_test_v2))\n",
        "print(\"\\n[V2] Stacked classification report:\\n\", classification_report(y_test_v2, stack_v2.predict(X_test_v2)))\n",
        "\n",
        "# --- F) Full-dataset predictions (new columns; your originals remain) ---\n",
        "df['predicted_season_tfidf_svm'] = svm_v2.predict(X_v2)\n",
        "df['predicted_season_stack'] = stack_v2.predict(X_v2)\n",
        "\n",
        "# Optional: compare old vs new on the first 20 rows\n",
        "cols_to_show = [\n",
        "    'title','designer','top_notes','middle_notes','base_notes',\n",
        "    'combined_notes','combined_notes_plus',\n",
        "    'predicted_season',                # from your original run\n",
        "    'predicted_season_tfidf_svm',      # new calibrated SVM on TF-IDF+features\n",
        "    'predicted_season_stack'           # new stacked ensemble\n",
        "]\n",
        "display(df[cols_to_show].head(20))\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# === MINI PERSONALIZER (2–3 Qs)\n",
        "# ===============================\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1) Define 3 fast questions and answer choices\n",
        "QUESTIONS = {\n",
        "    \"event\": {\n",
        "        \"prompt\": \"Where are you wearing this?\",\n",
        "        \"choices\": [\"Date night\", \"Family gathering\", \"School/Work\", \"Party/Going out\"]\n",
        "    },\n",
        "    \"vibe\": {\n",
        "        \"prompt\": \"Pick a vibe:\",\n",
        "        \"choices\": [\"Sweet/Comforting\", \"Fresh/Clean\", \"Woody/Spicy\", \"Floral/Soft\"]\n",
        "    },\n",
        "    \"strength\": {\n",
        "        \"prompt\": \"Projection preference?\",\n",
        "        \"choices\": [\"Skin-scent\", \"Moderate\", \"Strong\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2) Map answers -> preferred notes and season nudges\n",
        "ANSWER_TO_NOTES = {\n",
        "    \"event\": {\n",
        "        \"Date night\":         {\"notes\": [\"Vanilla\",\"Amber\",\"Tonka\",\"Lavender\",\"Cinnamon\"], \"season_bias\": [\"Fall\",\"Winter\"]},\n",
        "        \"Family gathering\":   {\"notes\": [\"Citrus\",\"Bergamot\",\"Grapefruit\",\"Green\",\"Neroli\"], \"season_bias\": [\"Spring\",\"Summer\"]},\n",
        "        \"School/Work\":        {\"notes\": [\"Fresh\",\"Lavender\",\"Tea\",\"Aromatic\",\"Woody\"], \"season_bias\": [\"Spring\",\"Fall\"]},\n",
        "        \"Party/Going out\":    {\"notes\": [\"Sweet\",\"Amber\",\"Oud\",\"Leather\",\"Pineapple\"], \"season_bias\": [\"Fall\",\"Winter\"]},\n",
        "    },\n",
        "    \"vibe\": {\n",
        "        \"Sweet/Comforting\":   {\"notes\": [\"Vanilla\",\"Tonka\",\"Amber\",\"Cinnamon\",\"Cocoa\"], \"season_bias\": [\"Fall\",\"Winter\"]},\n",
        "        \"Fresh/Clean\":        {\"notes\": [\"Citrus\",\"Marine\",\"Mint\",\"Green\",\"Neroli\"], \"season_bias\": [\"Spring\",\"Summer\"]},\n",
        "        \"Woody/Spicy\":        {\"notes\": [\"Sandalwood\",\"Cedar\",\"Cardamom\",\"Pepper\",\"Incense\"], \"season_bias\": [\"Fall\"]},\n",
        "        \"Floral/Soft\":        {\"notes\": [\"Jasmine\",\"Rose\",\"Iris\",\"Peony\",\"Lily\"], \"season_bias\": [\"Spring\"]},\n",
        "    },\n",
        "    \"strength\": {\n",
        "        \"Skin-scent\":         {\"notes\": [\"Musk\",\"Iris\",\"Tea\",\"Soft Floral\"], \"season_bias\": []},\n",
        "        \"Moderate\":           {\"notes\": [], \"season_bias\": []},\n",
        "        \"Strong\":             {\"notes\": [\"Amber\",\"Oud\",\"Leather\",\"Incense\"], \"season_bias\": [\"Fall\",\"Winter\"]},\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3) Utility: safe lowercase title-cased matching against your canonicalized notes\n",
        "def _canon_list(lst):\n",
        "    return [str(x).strip() for x in (lst or []) if isinstance(x, str) and str(x).strip()]\n",
        "\n",
        "def _notes_string(lst):\n",
        "    return \" \".join(_canon_list(lst))\n",
        "\n",
        "# 4) Ask questions (interactive) OR pass answers in programmatically\n",
        "def ask_user(min_questions=2):\n",
        "    answers = {}\n",
        "    for k in [\"event\", \"vibe\", \"strength\"][:max(2, min_questions)]:\n",
        "        ch = QUESTIONS[k][\"choices\"]\n",
        "        prompt = f\"{QUESTIONS[k]['prompt']} {ch}  -> \"\n",
        "        # Basic console input; you can replace with a UI later\n",
        "        ans = input(prompt).strip()\n",
        "        # fall back to first choice if mismatch\n",
        "        if ans not in ch:\n",
        "            ans = ch[0]\n",
        "        answers[k] = ans\n",
        "    return answers\n",
        "\n",
        "# 5) Convert answers into a preference profile (notes + season weights)\n",
        "def answers_to_profile(answers):\n",
        "    target_notes = []\n",
        "    season_weights = {s: 0.0 for s in [\"Spring\",\"Summer\",\"Fall\",\"Winter\"]}\n",
        "\n",
        "    for q, ans in answers.items():\n",
        "        m = ANSWER_TO_NOTES.get(q, {}).get(ans, {\"notes\":[], \"season_bias\":[]})\n",
        "        target_notes += m[\"notes\"]\n",
        "        for sb in m[\"season_bias\"]:\n",
        "            season_weights[sb] += 1.0\n",
        "\n",
        "    # Normalize season weights; if none selected, trust model predictions as-is\n",
        "    total = sum(season_weights.values())\n",
        "    if total == 0:\n",
        "        season_weights = {s: 0.0 for s in season_weights}\n",
        "    else:\n",
        "        season_weights = {k: v/total for k,v in season_weights.items()}\n",
        "\n",
        "    return list(dict.fromkeys(target_notes)), season_weights\n",
        "\n",
        "# 6) Rank perfumes by: (A) note similarity to the user prefs (tf-idf),\n",
        "#    and (B) season agreement with your model predictions\n",
        "def recommend_from_answers(answers, top_k=5):\n",
        "    target_notes, season_w = answers_to_profile(answers)\n",
        "\n",
        "    # Build a tiny \"query doc\" for tf-idf using your existing vectorizer (tfidf, built above)\n",
        "    query_doc = \" \".join(target_notes) if target_notes else \"\"\n",
        "    if query_doc and 'tfidf' in globals():\n",
        "        q_vec = tfidf.transform([query_doc])\n",
        "        sims = cosine_similarity(q_vec, X_tfidf).ravel()\n",
        "    else:\n",
        "        # fallback: simple Jaccard overlap on lists\n",
        "        tn = set(target_notes)\n",
        "        sims = df['combined_notes_plus'].apply(lambda n: len(tn.intersection(set(n or []))) / max(1, len(tn.union(set(n or [])))) ).to_numpy()\n",
        "\n",
        "    # Season agreement score: 1 if matches the user-biased “best” season, else fractional by weights\n",
        "    # If no bias, we’ll give 0.5 to any match with model’s predicted season; else weight by bias\n",
        "    model_season = df.get('predicted_season_stack', df.get('predicted_season_tfidf_svm', df.get('predicted_season')))\n",
        "    model_season = model_season.fillna(df['season'])  # fallback to labeled season if present\n",
        "    season_score = []\n",
        "    if any(season_w.values()):\n",
        "        for s in model_season:\n",
        "            season_score.append(season_w.get(s, 0.0))\n",
        "    else:\n",
        "        # light reward for agreeing with the model if no bias\n",
        "        for _ in model_season:\n",
        "            season_score.append(0.5)\n",
        "    season_score = np.array(season_score, dtype=float)\n",
        "\n",
        "    # Combine: simple weighted blend (feel free to tune)\n",
        "    score = 0.7 * sims + 0.3 * season_score\n",
        "\n",
        "    # Build output frame\n",
        "    out = df.assign(\n",
        "        _score = score,\n",
        "        _sim = sims,\n",
        "        _season_pref = season_score\n",
        "    ).sort_values(\"_score\", ascending=False)\n",
        "\n",
        "    cols = [\n",
        "        \"title\",\"designer\",\"predicted_season\",\"predicted_season_tfidf_svm\",\n",
        "        \"predicted_season_stack\",\"top_notes\",\"middle_notes\",\"base_notes\",\"combined_notes_plus\",\"_score\"\n",
        "    ]\n",
        "    cols = [c for c in cols if c in out.columns]\n",
        "    return out[cols].head(top_k)\n",
        "\n",
        "# 7) Example usage:\n",
        "#    a) Interactive (console in Jupyter/Colab/terminal):\n",
        "# answers = ask_user(min_questions=3)\n",
        "# recs = recommend_from_answers(answers, top_k=5)\n",
        "# display(recs)\n",
        "\n",
        "#    b) Programmatic (no input calls):\n",
        "example_answers = {\n",
        "    \"event\": \"Date night\",\n",
        "    \"vibe\": \"Sweet/Comforting\",\n",
        "    \"strength\": \"Moderate\"\n",
        "}\n",
        "recs = recommend_from_answers(example_answers, top_k=5)\n",
        "display(recs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
